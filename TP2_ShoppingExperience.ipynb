{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importations de bibliothèques \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement des deux tables de la base de données\n",
    "orders_distance_stores_softmax = pd.read_csv(\"orders_distance_stores_softmax.csv\")\n",
    "orders_products__prior_specials = pd.read_csv(\"order_products__prior_specials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_distance_stores_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_products__prior_specials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concaténation des deux tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir l'ensemble des noms de colonnes uniques\n",
    "colonnes_uniques = set(orders_distance_stores_softmax.columns).union(set(orders_products__prior_specials.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la concaténation, nous avons utilisé la méthode \"merge\" en se basant sur l'attribut commun des deux tables \"order_id\". Nous avons fixé la méthode de jointure à \"inner\" pour obtenir uniquement les lignes qui ont des correspondances dans les deux tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combiner les deux tables de la base de données\n",
    "data = orders_products__prior_specials.merge(orders_distance_stores_softmax, on='order_id', how='inner', suffixes=('', '_dup'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre ensemble de données contient 1172312 et 15 colonnes dont les colonnes \"Unnamed\". Nous allons les supprimer (non pertinents pour notre étude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Unnamed: 0', 'Unnamed: 0_dup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons donc 13 attributs donc l'attribut à prédire \"reordered\". Nous allons afficher le nombre d'échantillons pour chaque classe de l'attribut. Cette information sera pertinente pour l'analyse des résultats de prédiction. En cas de déséquilibre, il faut choisir des métriques spécifiques comme F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['reordered'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réduction des données (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de réduire nos données, nous allons les pourcentages des valeurs manquantes pour optimiser notre nouvel sous-ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la moyenne des valeurs manquantes pour chaque colonne\n",
    "pourcentage_valeurs_manquantes = (data.isnull().mean() * 100).round(2)\n",
    "\n",
    "# Convertir le résultat en DataFrame pour une meilleure présentation\n",
    "tableau_pourcentage_valeurs_manquantes = pd.DataFrame(pourcentage_valeurs_manquantes, columns=['Pourcentage de valeurs manquantes'])\n",
    "\n",
    "tableau_pourcentage_valeurs_manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque nous avons plus de 1 milions de lignes (échantillons), nous nous permettons de supprimer les lignes où la valeur 'days_since_prior_order' est manquante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des lignes où 'days_since_prior_order' est null\n",
    "data = data.dropna(subset=['days_since_prior_order'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour générer un sous ensemble de 5% des données, nous avons utilisé la méthode \"train_test_split\". Nous avons fait une stratification par rapport au \"user_id\" parce que notre prédiction de l'attribut \"reordered\" concerne aussi l'utilisateur (s'il a refait la même commande) donc nous avons voulu assurer qu'il y a un nombre équilibré et varié des utilisateurs (et leurs commandes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratification avec les IDs des utilisateurs\n",
    "_, sampled_df = train_test_split(data, test_size=0.05, stratify=data['user_id'], random_state=42)\n",
    "\n",
    "# Sous ensemble obtenu\n",
    "print(sampled_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre sous-ensemble contient 57923 échantillons et 12 prédicteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#afficher quelques statistiques sur les attributs du sous ensemble obtenu\n",
    "sampled_stats = sampled_df.describe()\n",
    "print(sampled_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons vérifier qu'il y a un nombre raisonnable des commandes par utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la taille de chaque groupe d'user_id\n",
    "group_sizes = sampled_df.groupby('user_id').size()\n",
    "\n",
    "# Tentative d'afficher tous les résultats\n",
    "# print(group_sizes.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = group_sizes.index.to_numpy()\n",
    "user_id = np.reshape(user_id, (user_id.shape[0], 1))\n",
    "values = group_sizes.to_numpy()\n",
    "values = np.reshape(values, (values.shape[0], 1))\n",
    "distributions = pd.DataFrame(np.hstack((user_id, values)), columns=[\"user_id\", \"values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'distributions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdistributions\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      2\u001b[0m                     xlabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m\"\u001b[39m, xticks\u001b[38;5;241m=\u001b[39m[], \n\u001b[0;32m      3\u001b[0m                     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistributions de la representativité des utilisateurs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m                     legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m                     )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'distributions' is not defined"
     ]
    }
   ],
   "source": [
    "distributions.plot.bar(x=\"user_id\", y=\"values\", \n",
    "                    xlabel=\"User\", xticks=[], \n",
    "                    title=\"Distributions de la representativité des utilisateurs\",\n",
    "                    legend=None,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodage des variables catégorielles (eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Identification des colonnes catégorielles\n",
    "variables_categorielles = sampled_df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Boucle sur chaque colonne catégorielle pour appliquer le LabelEncoder directement sur la colonne originale\n",
    "for col in variables_categorielles:\n",
    "    # Application du LabelEncoder à la colonne et écrasement de la colonne originale avec la version encodée\n",
    "    sampled_df[col] = label_encoder.fit_transform(sampled_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons que \"eval_set\" a une seule valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df['eval_set'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons convertir les valeurs de l'attribut 'order_hour_of_day' pour qu'elles aient la même unité que l'attribut 'days_since_prior_order'. Nous voulons que les deux attributs soient exprimés en jours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les heures en jours (1 jour = 24 heures) pour 'order_hour_of_day'\n",
    "sampled_df['order_hour_of_day'] = sampled_df['order_hour_of_day'] / 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification des valeurs dupliquées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_df = sampled_df[sampled_df.duplicated()]\n",
    "duplicates_df_sorted = duplicates_df.sort_values(by=['order_id'])\n",
    "print(f\"Nombre de valeurs dupliquées dans nos données \\n{duplicates_df_sorted.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous avons vérifié que nos échantillons ne soient pas dupliqués."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des colonnes numériques\n",
    "colonnes_numeriques = sampled_df.select_dtypes(include=['float64', 'int64', 'int32']).columns\n",
    "\n",
    "# Définition de la taille de la grille\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(colonnes_numeriques) / n_cols))\n",
    "\n",
    "# Création des figures de boxplot avec ajustement de la taille\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 6))  # Ajustez la largeur et la hauteur selon vos préférences\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(colonnes_numeriques):\n",
    "    sns.boxplot(y=sampled_df[col], ax=axes[i])\n",
    "    axes[i].set_title(f'Boxplot de {col}')\n",
    "    axes[i].set_ylabel(col)\n",
    "\n",
    "# Masquer les axes supplémentaires s'il y en a\n",
    "for j in range(i + 1, n_rows * n_cols):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des visualisations des boxplots : \n",
    "\n",
    "Boxplot de store_id : Les identifiants de magasin montrent une certaine variabilité, mais la distribution est centrée, ce qui indique que les commandes proviennent d'un groupe relativement cohérent de magasins.\n",
    "\n",
    "Boxplot de order_number : Les numéros de commande semblent avoir une distribution étendue avec la médiane vers le bas de la gamme, ce qui pourrait indiquer que de nombreux utilisateurs sont dans les premières étapes de l'historique de commande.\n",
    "\n",
    "Boxplot de days_since_prior_order : Cette variable a une médiane proche de 0 avec des valeurs étendues vers le haut, ce qui indique des variations dans le temps écoulé depuis la dernière commande.\n",
    "\n",
    "Boxplot de product_id : Les identifiants de produit sont distribués sur une large gamme avec quelques valeurs aberrantes, ce qui indique une grande variété de produits commandés.\n",
    "\n",
    "Boxplot de special : Cette variable a une médiane à 0 et quelques valeurs aberrantes, ce qui suggère que des offres spéciales ne sont appliquées que dans un nombre limité de cas.\n",
    "\n",
    "Boxplot de distance : Il y a des valeurs aberrantes, ce qui indique que bien que la plupart des utilisateurs soient à une distance moyenne des magasins, certains sont nettement plus éloignés.\n",
    "\n",
    "Boxplot de order_dow : La distribution semble régulière sur toute la semaine sans valeurs aberrantes, suggérant une tendance uniforme dans les jours de passation des commandes.\n",
    "\n",
    "Boxplot de add_to_cart_order : Il y a une grande variété dans l'ordre d'ajout des produits au panier, avec de nombreuses valeurs aberrantes, ce qui pourrait indiquer que certains produits ont tendance à être ajoutés au panier avant d'autres.\n",
    "\n",
    "Boxplot de user_id : Les identifiants d'utilisateur ont une large distribution, ce qui suggère une grande base d'utilisateurs.\n",
    "\n",
    "Boxplot de eval_set : Le boxplot n'est pas informatif ici puisque l'attribut contient une seule valeur.\n",
    "\n",
    "Boxplot de order_hour_of_day : La distribution des heures de commande est centrée avec quelques valeurs aberrantes, indiquant des moments privilégiés pour passer commande au cours de la journée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce contexte de ce tp, vous souhaitez prédire si un produit sera réordonné (reordered étant la variable cible) dans une future commande en fonction de l'historique d'achat d'un utilisateur. Basé sur cette objectif, voici les attributs que nous pensons ne pas avoir de valeur ajoutée pour la prédiction et donc être des candidats à l'élimination :\n",
    "\n",
    "- order_id : C'est un identifiant unique pour chaque commande, il n'aide pas directement à prédire le réordonnancement. \n",
    "\n",
    "- eval_set : Il s'agit simplement d'une variable technique indiquant l'appartenance des données à un ensemble d'entraînement, de validation ou de test et qui garde la même valeur pour toutes les échantillons de notre ensemble de données. Elle ne doit pas être incluse dans l'entraînement du modèle pour éviter la fuite de données.\n",
    "\n",
    "- store_id : L'identifiant du magasin n'a pas de relation avec la probabilité de réordonner un produit, cet attribut est à écarter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'attribut \"user_id\" en soi n'est pas une caractéristique prédictive directe, mais il est essentiel pour grouper les données et générer des caractéristiques comportementales significatives au niveau de l'utilisateur qui peuvent grandement améliorer la performance de votre modèle de prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = sampled_df.drop(columns = ['store_id', 'order_id', 'eval_set'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division des données en ensembles Train et Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sampled_df.drop('reordered', axis=1)  # Caractéristiques\n",
    "y = sampled_df['reordered']  # Variable cible\n",
    "\n",
    "# Séparation des données en ensemble d'entraînement (80%) et de test (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = X_train.select_dtypes(include=['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réglage des paramètres pour afficher 3 figures par ligne\n",
    "n_cols = 3  # Nombre de colonnes par ligne dans la grille de subplots\n",
    "n_rows = int(np.ceil(len(numeric_columns) / n_cols))  # Calcul du nombre de lignes nécessaires\n",
    "\n",
    "# Création d'une figure et de subplots avec la grille spécifiée\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
    "\n",
    "# Aplatir le tableau d'axes pour une itération facile si nécessaire\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "# Visualisation avec des histogrammes pour chaque colonne numérique\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    sns.histplot(X_train[col], color='blue', label='Train', kde=True, ax=axes_flat[i])\n",
    "    sns.histplot(X_test[col], color='orange', label='Test', kde=True, ax=axes_flat[i])\n",
    "    axes_flat[i].set_title(f'Distribution de {col} pour train et test')\n",
    "    axes_flat[i].legend()\n",
    "\n",
    "# Si le nombre total de colonnes numériques n'est pas un multiple de 3, masquer les axes vides\n",
    "for j in range(i+1, len(axes_flat)):\n",
    "    axes_flat[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une figure avec la grille de subplots pour des courbes de densité\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 4))\n",
    "\n",
    "# Parcourir chaque colonne numérique et tracer des courbes de densité pour les ensembles train et test\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    row = i // n_cols\n",
    "    col_index = i % n_cols\n",
    "    ax = axes[row, col_index]\n",
    "\n",
    "    # Tracer la courbe de densité pour l'ensemble d'entraînement\n",
    "    sns.kdeplot(data=X_train, x=col, fill=True, ax=ax, label='Train', color='blue', alpha=0.5)\n",
    "\n",
    "    # Tracer la courbe de densité pour l'ensemble de test\n",
    "    sns.kdeplot(data=X_test, x=col, fill=True, ax=ax, label='Test', color='orange', alpha=0.5)\n",
    "\n",
    "    ax.set_title(f'Distribution de {col}')\n",
    "    ax.legend()\n",
    "\n",
    "# Cacher les subplots vides s'il y en a\n",
    "for j in range(i+1, n_rows*n_cols):\n",
    "    axes.flat[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les courbes montrent que les ensembles d'entraînement et de test ont des distributions cohérentes et comparables pour la majorité des attributs examinées. Cela signifie que la séparation des données a été bien réalisée, offrant ainsi une bonne représentativité et permettant d'anticiper que les modèles de machine learning entraînés sur l'ensemble d'entraînement devraient avoir des performances similaires sur l'ensemble de test. Les pics et creux correspondants dans les courbes de densité de product_id, add_to_cart_order, et days_since_prior_order reflètent des tendances spécifiques de comportement d'achat qui sont préservées entre les ensembles. Les caractéristiques périodiques comme order_dow et les pics d'heures dans order_hour_of_day sont particulièrement remarquables, révélant des schémas d'achat récurrents sur les jours et les heures qui pourraient être significatifs pour la prédiction du réordonnancement. En résumé, ces visualisations fournissent une confirmation visuelle que la séparation des données maintient l'intégrité statistique nécessaire pour une modélisation prédictive fiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour les données d'entraînement\n",
    "X_train.to_csv('x_train.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "\n",
    "# Pour les données de test\n",
    "X_test.to_csv('x_test.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "\n",
    "# Pour sample_df\n",
    "sampled_df.to_csv('final_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement des modèles (Question 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46338, 9) (46338, 1) (11585, 9) (11585, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "y_test = pd.read_csv('y_test.csv')\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisons la classe StandardScaler de sklearn pour normaliser les données, permettant ainsi de les mettre à la même échelle, ce qui améliorera les performances du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implémentation du CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Puisque la couche de sortie utilise deux neurones, nous devons lui fournir un vecteur binaire à deux dimensions, c'est-à-dire contenant soit 0 ou 1. Cependant, notre vecteur cible est unidimensionnel. Par conséquent, nous allons utiliser un encodeur OneHotEncoder pour le transformer en un vecteur avec les dimensions appropriées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46338, 2) (11585, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "y_train_ = np.reshape(y_train, (m_train, 1))\n",
    "y_test_ = np.reshape(y_test, (m_test, 1))\n",
    "\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "y_train_ = enc.fit_transform(y_train_)\n",
    "y_test_ = enc.fit_transform(y_test_)\n",
    "print(y_train_.shape, y_test_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons mettre en place un modèle de réseau de neurones convolutionnel (CNN) comprenant deux couches cachées utilisant la fonction d'activation relu, ainsi qu'une couche de sortie softmax composée de deux neurones, chacun représentant l'une des deux classes cibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7940 - f1_score: 0.5995 - loss: 0.4479\n",
      "Epoch 2/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8007 - f1_score: 0.6321 - loss: 0.4242\n",
      "Epoch 3/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8074 - f1_score: 0.6366 - loss: 0.4181\n",
      "Epoch 4/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8067 - f1_score: 0.6341 - loss: 0.4146\n",
      "Epoch 5/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8055 - f1_score: 0.6352 - loss: 0.4187\n",
      "Epoch 6/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8072 - f1_score: 0.6429 - loss: 0.4191\n",
      "Epoch 7/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8080 - f1_score: 0.6400 - loss: 0.4150\n",
      "Epoch 8/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8097 - f1_score: 0.6450 - loss: 0.4103\n",
      "Epoch 9/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8085 - f1_score: 0.6492 - loss: 0.4159\n",
      "Epoch 10/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8090 - f1_score: 0.6415 - loss: 0.4154\n",
      "Epoch 11/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8083 - f1_score: 0.6480 - loss: 0.4163\n",
      "Epoch 12/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8094 - f1_score: 0.6511 - loss: 0.4131\n",
      "Epoch 13/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8115 - f1_score: 0.6549 - loss: 0.4091\n",
      "Epoch 14/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8124 - f1_score: 0.6535 - loss: 0.4117\n",
      "Epoch 15/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8156 - f1_score: 0.6580 - loss: 0.4054\n",
      "Epoch 16/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8095 - f1_score: 0.6497 - loss: 0.4146\n",
      "Epoch 17/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8137 - f1_score: 0.6545 - loss: 0.4084\n",
      "Epoch 18/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8163 - f1_score: 0.6670 - loss: 0.4030\n",
      "Epoch 19/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8117 - f1_score: 0.6563 - loss: 0.4073\n",
      "Epoch 20/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8138 - f1_score: 0.6590 - loss: 0.4068\n",
      "Epoch 21/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.8159 - f1_score: 0.6624 - loss: 0.4042\n",
      "Epoch 22/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8147 - f1_score: 0.6664 - loss: 0.4037\n",
      "Epoch 23/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8201 - f1_score: 0.6714 - loss: 0.3992\n",
      "Epoch 24/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8169 - f1_score: 0.6768 - loss: 0.4007\n",
      "Epoch 25/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8180 - f1_score: 0.6658 - loss: 0.4012\n",
      "Epoch 26/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8180 - f1_score: 0.6799 - loss: 0.4008\n",
      "Epoch 27/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8195 - f1_score: 0.6744 - loss: 0.4000\n",
      "Epoch 28/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8231 - f1_score: 0.6868 - loss: 0.3906\n",
      "Epoch 29/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8197 - f1_score: 0.6787 - loss: 0.4004\n",
      "Epoch 30/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8244 - f1_score: 0.6898 - loss: 0.3936\n",
      "Epoch 31/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8221 - f1_score: 0.6863 - loss: 0.3985\n",
      "Epoch 32/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8218 - f1_score: 0.6840 - loss: 0.3941\n",
      "Epoch 33/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8244 - f1_score: 0.6900 - loss: 0.3945\n",
      "Epoch 34/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8264 - f1_score: 0.6915 - loss: 0.3907\n",
      "Epoch 35/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8269 - f1_score: 0.6921 - loss: 0.3880\n",
      "Epoch 36/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8270 - f1_score: 0.6942 - loss: 0.3886\n",
      "Epoch 37/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8263 - f1_score: 0.6905 - loss: 0.3900\n",
      "Epoch 38/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8279 - f1_score: 0.6969 - loss: 0.3860\n",
      "Epoch 39/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8355 - f1_score: 0.7097 - loss: 0.3780\n",
      "Epoch 40/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8284 - f1_score: 0.6942 - loss: 0.3871\n",
      "Epoch 41/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8318 - f1_score: 0.6969 - loss: 0.3812\n",
      "Epoch 42/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8313 - f1_score: 0.7039 - loss: 0.3822\n",
      "Epoch 43/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8268 - f1_score: 0.6945 - loss: 0.3818\n",
      "Epoch 44/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8343 - f1_score: 0.7103 - loss: 0.3785\n",
      "Epoch 45/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8311 - f1_score: 0.6999 - loss: 0.3790\n",
      "Epoch 46/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8336 - f1_score: 0.7026 - loss: 0.3755\n",
      "Epoch 47/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.8334 - f1_score: 0.7089 - loss: 0.3769\n",
      "Epoch 48/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8345 - f1_score: 0.7093 - loss: 0.3747\n",
      "Epoch 49/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.8322 - f1_score: 0.7063 - loss: 0.3778\n",
      "Epoch 50/50\n",
      "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8350 - f1_score: 0.7127 - loss: 0.3741\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "f1 = tf.keras.metrics.F1Score(average=None, threshold=None, name=\"f1_score\", dtype=None)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1])\n",
    "history = model.fit(x=X_train_scaled, y=y_train_, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x=X_test, y=y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label=\"La précision d'entrainement.\")\n",
    "plt.title('Courbe d\\'exactitude')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label=\"Perte d'entrainement\")\n",
    "plt.title('Courbe de perte')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perte')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous implémentons la méthode de classification Machine à vecteur de support, avec un gridsearch pour la recherche des meilleurs hyper-parametres (Le noyeau et la constante C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46338,) (11585,)\n"
     ]
    }
   ],
   "source": [
    "y_train_ = np.reshape(y_train, (-1,))\n",
    "y_test_ = np.reshape(y_test, (-1,))\n",
    "\n",
    "print(y_train_.shape, y_test_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentation de la méthode SVM de la librairie sklearn, avec la recherche d'hyper-paramêtres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46338, 9) (46338,)\n",
      "{'C': 0.1, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category = ConvergenceWarning)\n",
    "\n",
    "parameters = {'kernel': ('poly', 'rbf'), 'C': [0.1, 1, 10]}\n",
    "svc = svm.SVC(max_iter = 100)\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "clf.fit(X_train_scaler, y_train_)\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rapport de classification :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.19      0.19      2486\n",
      "           1       0.78      0.79      0.79      9099\n",
      "\n",
      "    accuracy                           0.66     11585\n",
      "   macro avg       0.49      0.49      0.49     11585\n",
      "weighted avg       0.66      0.66      0.66     11585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test_scaler)\n",
    "\n",
    "print(\"\\nRapport de classification :\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
