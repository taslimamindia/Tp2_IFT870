{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/taslimamindia/Tp2_IFT870.git"
      ],
      "metadata": {
        "id": "FnXLBvXT43SD",
        "outputId": "176acf45-fe9f-4edf-87b4-84c9471c8380",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Tp2_IFT870' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qq3CFnYu4111",
        "outputId": "87b2cc01-fc26-42a9-d208-613a9ccc7fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Name tf.RaggedTensorSpec has already been registered for class tensorflow.python.ops.ragged.ragged_tensor.RaggedTensorSpec.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f2b7eef611a2>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__operators__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meager_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmulti_process_runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menv\u001b[0m \u001b[0;31m# line: 456\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate\u001b[0m \u001b[0;31m# line: 365\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0min_main_process\u001b[0m \u001b[0;31m# line: 418\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/combinations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_all_reduce_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmulti_process_runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_server_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_device_ops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcross_device_ops_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_device_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/cross_device_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_device_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/cross_device_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvalue_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackprop_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/values.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstruct_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpacked_distributed_variable\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpacked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreduce_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mag_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mautograph_ctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdense_to_ragged_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdense_to_sparse_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/service/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    417\u001b[0m \"\"\"\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_dataset_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_service_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompression_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_server_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/ops/compression_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# ==============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_experimental_dataset_ops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mged_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource_variable_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mragged_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_logging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minternal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/ragged/ragged_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RaggedTensorSpec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2320\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtype_spec_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.RaggedTensorSpec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2321\u001b[0;31m class RaggedTensorSpec(\n\u001b[0m\u001b[1;32m   2322\u001b[0m     type_spec.BatchableTypeSpec, internal_types.RaggedTensorSpec):\n\u001b[1;32m   2323\u001b[0m   \u001b[0;34m\"\"\"Type specification for a `tf.RaggedTensor`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/type_spec_registry.py\u001b[0m in \u001b[0;36mdecorator_fn\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     57\u001b[0m                        (cls.__module__, cls.__name__, _TYPE_SPEC_TO_NAME[cls]))\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_NAME_TO_TYPE_SPEC\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m       raise ValueError(\"Name %s has already been registered for class %s.%s.\" %\n\u001b[0m\u001b[1;32m     60\u001b[0m                        (name, _NAME_TO_TYPE_SPEC[name].__module__,\n\u001b[1;32m     61\u001b[0m                         _NAME_TO_TYPE_SPEC[name].__name__))\n",
            "\u001b[0;31mValueError\u001b[0m: Name tf.RaggedTensorSpec has already been registered for class tensorflow.python.ops.ragged.ragged_tensor.RaggedTensorSpec."
          ]
        }
      ],
      "source": [
        "#importations de bibliothèques\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zClRGM14119"
      },
      "outputs": [],
      "source": [
        "# chargement des deux tables de la base de données\n",
        "orders_distance_stores_softmax = pd.read_csv(\"Tp2_IFT870/orders_distance_stores_softmax.csv\")\n",
        "orders_products__prior_specials = pd.read_csv(\"Tp2_IFT870/order_products__prior_specials.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHnTVhdk411-"
      },
      "outputs": [],
      "source": [
        "orders_distance_stores_softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOclUpy6412B"
      },
      "outputs": [],
      "source": [
        "orders_products__prior_specials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mmDoYL2412C"
      },
      "source": [
        "## Concaténation des deux tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK5Nzuj5412G"
      },
      "outputs": [],
      "source": [
        "# Obtenir l'ensemble des noms de colonnes uniques\n",
        "colonnes_uniques = set(orders_distance_stores_softmax.columns).union(set(orders_products__prior_specials.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXkF1Mcd412H"
      },
      "source": [
        "Pour la concaténation, nous avons utilisé la méthode \"merge\" en se basant sur l'attribut commun des deux tables \"order_id\". Nous avons fixé la méthode de jointure à \"inner\" pour obtenir uniquement les lignes qui ont des correspondances dans les deux tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07q5mPqL412I"
      },
      "outputs": [],
      "source": [
        "#combiner les deux tables de la base de données\n",
        "data = orders_products__prior_specials.merge(orders_distance_stores_softmax, on='order_id', how='inner', suffixes=('', '_dup'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDj4LyLW412J"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDBF5uxL412K"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyQ5qxFD412L"
      },
      "source": [
        "Notre ensemble de données contient 1172312 et 15 colonnes dont les colonnes \"Unnamed\". Nous allons les supprimer (non pertinents pour notre étude)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xcq_M7kT412L"
      },
      "outputs": [],
      "source": [
        "data = data.drop(columns=['Unnamed: 0', 'Unnamed: 0_dup'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp_k0QAt412M"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKLKvSPC412M"
      },
      "source": [
        "Nous avons donc 13 attributs donc l'attribut à prédire \"reordered\". Nous allons afficher le nombre d'échantillons pour chaque classe de l'attribut. Cette information sera pertinente pour l'analyse des résultats de prédiction. En cas de déséquilibre, il faut choisir des métriques spécifiques comme F1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg20BNli412N"
      },
      "outputs": [],
      "source": [
        "data['reordered'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxMbft4v412N"
      },
      "source": [
        "## Réduction des données (5%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG8qtZrc412O"
      },
      "source": [
        "Avant de réduire nos données, nous allons les pourcentages des valeurs manquantes pour optimiser notre nouvel sous-ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXeqb0bo412O"
      },
      "outputs": [],
      "source": [
        "# Calcul de la moyenne des valeurs manquantes pour chaque colonne\n",
        "pourcentage_valeurs_manquantes = (data.isnull().mean() * 100).round(2)\n",
        "\n",
        "# Convertir le résultat en DataFrame pour une meilleure présentation\n",
        "tableau_pourcentage_valeurs_manquantes = pd.DataFrame(pourcentage_valeurs_manquantes, columns=['Pourcentage de valeurs manquantes'])\n",
        "\n",
        "tableau_pourcentage_valeurs_manquantes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWGO6C2D412O"
      },
      "source": [
        "Puisque nous avons plus de 1m de lignes (échantillons), nous nous permettons de supprimer les lignes où la valeur 'days_since_prior_order' est manquante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y95iNDT412P"
      },
      "outputs": [],
      "source": [
        "# Suppression des lignes où 'days_since_prior_order' est null\n",
        "data = data.dropna(subset=['days_since_prior_order'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn2i9BQ_412P"
      },
      "source": [
        "Pour générer un sous ensemble de 5% des données, nous avons utilisé la méthode \"train_test_split\". Nous avons fait une stratification par rapport au \"user_id\" parce que notre prédiction de l'attribut \"reordered\" concerne aussi l'utilisateur (s'il a refait la même commande) donc nous avons voulu assurer qu'il y a un nombre équilibré et varié des utilisateurs (et leurs commandes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDG8ox1x412Q"
      },
      "outputs": [],
      "source": [
        "# stratification avec les IDs des utilisateurs\n",
        "_, sampled_df = train_test_split(data, test_size=0.05, stratify=data['user_id'], random_state=42)\n",
        "\n",
        "# Sous ensemble obtenu\n",
        "print(sampled_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHApcVue412Q"
      },
      "outputs": [],
      "source": [
        "sampled_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7EydoV8412Q"
      },
      "source": [
        "Notre sous-ensemble contient 57923 échantillons et 12 prédicteurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUt0cjmv412R"
      },
      "outputs": [],
      "source": [
        "#afficher quelques statistiques sur les attributs du sous ensemble obtenu\n",
        "sampled_stats = sampled_df.describe()\n",
        "print(sampled_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4ZbD2mc412Y"
      },
      "source": [
        "Nous allons vérifier qu'il y a un nombre raisonnable des commandes par utilisateur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfpKjWc9412Z"
      },
      "outputs": [],
      "source": [
        "# Calcul de la taille de chaque groupe d'user_id\n",
        "group_sizes = sampled_df.groupby('user_id').size()\n",
        "\n",
        "# Tentative d'afficher tous les résultats\n",
        "print(group_sizes.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDxYUoKU412Z"
      },
      "source": [
        "## Préparation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZMEVLnZ412a"
      },
      "source": [
        "Encodage des variables catégorielles (eval_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N24DTEoU412b"
      },
      "outputs": [],
      "source": [
        "# Initialisation du LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Identification des colonnes catégorielles\n",
        "variables_categorielles = sampled_df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Boucle sur chaque colonne catégorielle pour appliquer le LabelEncoder directement sur la colonne originale\n",
        "for col in variables_categorielles:\n",
        "    # Application du LabelEncoder à la colonne et écrasement de la colonne originale avec la version encodée\n",
        "    sampled_df[col] = label_encoder.fit_transform(sampled_df[col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T53gfKPe412b"
      },
      "outputs": [],
      "source": [
        "sampled_df.head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4mCEnZh412b"
      },
      "outputs": [],
      "source": [
        "#catégories pour les attributs de notre ensemble de données\n",
        "sampled_df.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpDdE82B412c"
      },
      "source": [
        "Nous remarquons que \"eval_set\" a une seule valeur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0qBy6kz412c"
      },
      "outputs": [],
      "source": [
        "#vérifier que le LabelEncoder a été fonctionnel\n",
        "sampled_df['eval_set'].dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXPZKi9T412d"
      },
      "source": [
        "Nous allons convertir les valeurs de l'attribut 'order_hour_of_day' pour qu'elles aient la même unité que l'attribut 'days_since_prior_order'. Nous voulons que les deux attributs soient exprimés en jours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anTdwX8z412d"
      },
      "outputs": [],
      "source": [
        "# Convertir les heures en jours (1 jour = 24 heures) pour 'order_hour_of_day'\n",
        "sampled_df['order_hour_of_day'] = sampled_df['order_hour_of_day'] / 24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLv3J97C412d"
      },
      "source": [
        "## Vérification des valeurs dupliquées"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSxMbf8m412d"
      },
      "outputs": [],
      "source": [
        "#vérifier qu'il n'y a pas de lignes (échantillons) dubliquées pour notre ensemble de données\n",
        "duplicates_df = sampled_df[sampled_df.duplicated()]\n",
        "duplicates_df_sorted = duplicates_df.sort_values(by=['order_id'])\n",
        "print(f\"Nombre de valeurs dupliquées dans nos données \\n{duplicates_df_sorted.sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcB9eccp412e"
      },
      "source": [
        "Dans cette partie, nous avons vérifié que nos échantillons ne soient pas dupliqués puisque tous les nombres sont à 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EWDSQaY412l"
      },
      "source": [
        "## Visualisation des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQk1s-oi412m"
      },
      "outputs": [],
      "source": [
        "# Sélection des colonnes numériques\n",
        "colonnes_numeriques = sampled_df.select_dtypes(include=['float64', 'int64', 'int32']).columns\n",
        "\n",
        "# Définition de la taille de la grille\n",
        "n_cols = 3\n",
        "n_rows = int(np.ceil(len(colonnes_numeriques) / n_cols))\n",
        "\n",
        "# Création des figures de boxplot avec ajustement de la taille\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 6))  # Ajuster la largeur et la hauteur selon vos préférences\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Traçage des boxplots pour les colonnes numériques\n",
        "for i, col in enumerate(colonnes_numeriques):\n",
        "    sns.boxplot(y=sampled_df[col], ax=axes[i])\n",
        "    axes[i].set_title(f'Boxplot de {col}')\n",
        "    axes[i].set_ylabel(col)\n",
        "\n",
        "# Masquer les axes supplémentaires s'il y en a\n",
        "for j in range(i + 1, n_rows * n_cols):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajniv47k412m"
      },
      "source": [
        "Analyse des visualisations des boxplots :\n",
        "\n",
        "Boxplot de store_id : Les identifiants de magasin montrent une certaine variabilité, mais la distribution est centrée, ce qui indique que les commandes proviennent d'un groupe relativement cohérent de magasins.\n",
        "\n",
        "Boxplot de order_number : Les numéros de commande semblent avoir une distribution étendue avec la médiane vers le bas de la gamme, ce qui pourrait indiquer que de nombreux utilisateurs sont dans les premières étapes de l'historique de commande.\n",
        "\n",
        "Boxplot de days_since_prior_order : Cette variable a une médiane proche de 0 avec des valeurs étendues vers le haut, ce qui indique des variations dans le temps écoulé depuis la dernière commande.\n",
        "\n",
        "Boxplot de product_id : Les identifiants de produit sont distribués sur une large gamme avec quelques valeurs aberrantes, ce qui indique une grande variété de produits commandés.\n",
        "\n",
        "Boxplot de special : Cette variable a une médiane à 0 et quelques valeurs aberrantes, ce qui suggère que des offres spéciales ne sont appliquées que dans un nombre limité de cas.\n",
        "\n",
        "Boxplot de distance : Il y a des valeurs aberrantes, ce qui indique que bien que la plupart des utilisateurs soient à une distance moyenne des magasins, certains sont nettement plus éloignés.\n",
        "\n",
        "Boxplot de order_dow : La distribution semble régulière sur toute la semaine sans valeurs aberrantes, suggérant une tendance uniforme dans les jours de passation des commandes.\n",
        "\n",
        "Boxplot de add_to_cart_order : Il y a une grande variété dans l'ordre d'ajout des produits au panier, avec de nombreuses valeurs aberrantes, ce qui pourrait indiquer que certains produits ont tendance à être ajoutés au panier avant d'autres.\n",
        "\n",
        "Boxplot de user_id : Les identifiants d'utilisateur ont une large distribution, ce qui suggère une grande base d'utilisateurs.\n",
        "\n",
        "Boxplot de eval_set : Le boxplot n'est pas informatif ici puisque l'attribut contient une seule valeur.\n",
        "\n",
        "Boxplot de order_hour_of_day : La distribution des heures de commande est centrée avec quelques valeurs aberrantes, indiquant des moments privilégiés pour passer commande au cours de la journée."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCL4J8IV412n"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofTYc1aV412n"
      },
      "source": [
        "Dans ce contexte de ce tp, vous souhaitez prédire si un produit sera réordonné (reordered étant la variable cible) dans une future commande en fonction de l'historique d'achat d'un utilisateur. Basé sur cette objectif, voici les attributs que nous pensons ne pas avoir de valeur ajoutée pour la prédiction et donc être des candidats à l'élimination :\n",
        "\n",
        "- order_id : C'est un identifiant unique pour chaque commande, il n'aide pas directement à prédire le réordonnancement.\n",
        "\n",
        "- eval_set : Il s'agit simplement d'une variable technique indiquant l'appartenance des données à un ensemble d'entraînement, de validation ou de test et qui garde la même valeur pour toutes les échantillons de notre ensemble de données. Elle ne doit pas être incluse dans l'entraînement du modèle pour éviter la fuite de données.\n",
        "\n",
        "- store_id : L'identifiant du magasin n'a pas de relation avec la probabilité de réordonner un produit, cet attribut est à écarter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXA1IV2F412n"
      },
      "source": [
        "L'attribut \"user_id\" en soi n'est pas une caractéristique prédictive directe, mais il est essentiel pour grouper les données et générer des caractéristiques comportementales significatives au niveau de l'utilisateur qui peuvent grandement améliorer la performance de votre modèle de prédiction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgg_UhYz412o"
      },
      "outputs": [],
      "source": [
        "# Suppression des attributs en question\n",
        "sampled_df = sampled_df.drop(columns = ['store_id', 'order_id', 'eval_set'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrJQF3Jc412o"
      },
      "source": [
        "## Normalisation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Foh24Y412o"
      },
      "source": [
        "Cette étape est importante et nécessaire pour l'entraînement de la majorité des modèles de Machine et/ou Deep Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-iB4yJe412o"
      },
      "outputs": [],
      "source": [
        "# Préparation des ensembles X et y\n",
        "X = sampled_df.drop('reordered', axis=1)\n",
        "y = sampled_df['reordered']\n",
        "\n",
        "# Création de l'objet StandardScaler pour la normalisation\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Ajustement du scaler sur les données puis transformation\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convertir X_scaled de nouveau en DataFrame pour garder les noms de colonne\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSSAYYRk412p"
      },
      "source": [
        "## Division des données en ensembles Train et Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwBo0t0E412p"
      },
      "outputs": [],
      "source": [
        "# Séparation des données en ensemble d'entraînement (80%) et de test (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYYY-AQx412p"
      },
      "outputs": [],
      "source": [
        "# Sélection des colonnes numériques pour des visualisations\n",
        "numeric_columns = X_train.select_dtypes(include=['float64', 'int64']).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIPw9cKb412q"
      },
      "outputs": [],
      "source": [
        "# Réglage des paramètres pour afficher 3 figures par ligne\n",
        "n_cols = 3  # Nombre de colonnes par ligne dans la grille de subplots\n",
        "n_rows = int(np.ceil(len(numeric_columns) / n_cols))  # Calcul du nombre de lignes nécessaires\n",
        "\n",
        "# Création d'une figure et de subplots avec la grille spécifiée\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
        "\n",
        "# Aplatir le tableau d'axes pour une itération facile si nécessaire\n",
        "axes_flat = axes.flatten()\n",
        "\n",
        "# Visualisation avec des histogrammes pour chaque colonne numérique\n",
        "for i, col in enumerate(numeric_columns):\n",
        "    sns.histplot(X_train[col], color='blue', label='Train', kde=True, ax=axes_flat[i])\n",
        "    sns.histplot(X_test[col], color='orange', label='Test', kde=True, ax=axes_flat[i])\n",
        "    axes_flat[i].set_title(f'Distribution de {col} pour train et test')\n",
        "    axes_flat[i].legend()\n",
        "\n",
        "# Si le nombre total de colonnes numériques n'est pas un multiple de 3, masquer les axes vides\n",
        "for j in range(i+1, len(axes_flat)):\n",
        "    axes_flat[j].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP3m1avf412q"
      },
      "outputs": [],
      "source": [
        "# Création d'une figure avec la grille de subplots pour des courbes de densité\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 4))\n",
        "\n",
        "# Parcourir chaque colonne numérique et tracer des courbes de densité pour les ensembles train et test\n",
        "for i, col in enumerate(numeric_columns):\n",
        "    row = i // n_cols\n",
        "    col_index = i % n_cols\n",
        "    ax = axes[row, col_index]\n",
        "\n",
        "    # Tracer la courbe de densité pour l'ensemble d'entraînement\n",
        "    sns.kdeplot(data=X_train, x=col, fill=True, ax=ax, label='Train', color='blue', alpha=0.5)\n",
        "\n",
        "    # Tracer la courbe de densité pour l'ensemble de test\n",
        "    sns.kdeplot(data=X_test, x=col, fill=True, ax=ax, label='Test', color='orange', alpha=0.5)\n",
        "\n",
        "    ax.set_title(f'Distribution de {col}')\n",
        "    ax.legend()\n",
        "\n",
        "# Cacher les subplots vides s'il y en a\n",
        "for j in range(i+1, n_rows*n_cols):\n",
        "    axes.flat[j].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6J3adni412r"
      },
      "source": [
        "Les courbes montrent que les ensembles d'entraînement et de test ont des distributions cohérentes et comparables pour la majorité des attributs examinées. Cela signifie que la séparation des données a été bien réalisée, offrant ainsi une bonne représentativité et permettant d'anticiper que les modèles de machine learning entraînés sur l'ensemble d'entraînement devraient avoir des performances similaires sur l'ensemble de test. Les pics et creux correspondants dans les courbes de densité de product_id, add_to_cart_order, et days_since_prior_order reflètent des tendances spécifiques de comportement d'achat qui sont préservées entre les ensembles. Les caractéristiques périodiques comme order_dow et les pics d'heures dans order_hour_of_day sont particulièrement remarquables, révélant des schémas d'achat récurrents sur les jours et les heures qui pourraient être significatifs pour la prédiction du réordonnancement. En résumé, ces visualisations fournissent une confirmation visuelle que la séparation des données maintient l'intégrité statistique nécessaire pour une modélisation prédictive fiable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U41DItdn412r"
      },
      "source": [
        "# Sauvegarde des fichiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lnEAw6h412r"
      },
      "outputs": [],
      "source": [
        "# Pour les données d'entraînement\n",
        "X_train.to_csv('x_train_final.csv', index=False)\n",
        "y_train.to_csv('y_train_final.csv', index=False)\n",
        "\n",
        "# Pour les données de test\n",
        "X_test.to_csv('x_test_final.csv', index=False)\n",
        "y_test.to_csv('y_test_final.csv', index=False)\n",
        "\n",
        "# Pour sample_df\n",
        "sampled_df.to_csv('final_data_1.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioKOcdRG412s"
      },
      "source": [
        "## Entraînement des modèles (Question 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddizF8vk412s"
      },
      "source": [
        "Avant d'entraîner nos données sur des modèles, nous allons voir la distribution des classes de l'attribut à prédire (reordered)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5BsohjD412s"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV8yXn5T412t"
      },
      "outputs": [],
      "source": [
        "y_test.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gki8CF6-412u"
      },
      "source": [
        "Nous avons choisi de ne pas appliquer de méthodes spécifiques pour équilibrer les classes de l'attribut à prédire. Nous traitons un problème de prédiction de re-commande d'un produit donc il est important que nous gardons les tendances telles qu'elles sont sans modification externe."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Puisque la couche de sortie utilise deux neurones, nous devons lui fournir un vecteur binaire à deux dimensions, c'est-à-dire contenant soit 0 ou 1. Cependant, notre vecteur cible est unidimensionnel. Par conséquent, nous allons utiliser un encodeur OneHotEncoder pour le transformer en un vecteur avec les dimensions appropriées."
      ],
      "metadata": {
        "id": "DnaDKUpUEihX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "y_train_t, y_test_t = y_train, y_test\n",
        "y_train_ = np.reshape(y_train, (-1, 1))\n",
        "y_test_ = np.reshape(y_test, (-1, 1))\n",
        "\n",
        "enc = OneHotEncoder(sparse_output=False)\n",
        "y_train = enc.fit_transform(y_train_)\n",
        "y_test = enc.fit_transform(y_test_)\n",
        "print(y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "HQ1rzkWVCRvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhUIdB8412u"
      },
      "source": [
        "Implémentation du CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVnpPo1k412u"
      },
      "outputs": [],
      "source": [
        "# chaque exemple de notre ensemble de données a N caractéristiques\n",
        "# Remodeler les données pour le CNN 1D\n",
        "n_features = X_train.shape[1]  # N sera le nombre total de caractéristiques\n",
        "X_train_reshaped = X_train.values.reshape(-1, n_features, 1)\n",
        "X_test_reshaped = X_test.values.reshape(-1, n_features, 1)\n",
        "print(X_train_reshaped.shape, X_test_reshaped.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC2oX526412v"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "\n",
        "# Initialiser le modèle séquentiel\n",
        "model = Sequential()\n",
        "\n",
        "# Ajouter la première couche convolutive 1D\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_features, 1)))\n",
        "\n",
        "# Ajouter une deuxième couche convolutive 1D\n",
        "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "\n",
        "# Ajouter une autre couche de MaxPooling\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Aplatir les sorties pour les rendre compatibles avec les couches denses\n",
        "model.add(Flatten())\n",
        "\n",
        "# Ajouter une couche dense cachée\n",
        "model.add(Dense(100, activation='relu'))\n",
        "\n",
        "# Ajouter la couche de sortie\n",
        "model.add(Dense(2, activation='softmax'))  # Utiliser 'sigmoid' pour la classification binaire\n",
        "\n",
        "# Afficher le résumé du modèle\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xndfprrp412v"
      },
      "source": [
        "Nous utilisons le score f1 car nous avons des classes non équilibrées et dans ce cas, la précision (accuracy) ne permet pas de capturer la vraie performance du modèle. Et,  car il assure que les modèles doivent bien performer à la fois sur les classes majoritaires et minoritaires pour obtenir un score élevé.\n",
        "En effet, dans les jeux de données déséquilibrés, améliorer la précision peut souvent se faire au détriment du rappel, et vice versa. Par exemple, un modèle pourrait prédire très conservativement les cas positifs (pour être sûr à 100% de leur positivité), augmentant ainsi la précision mais réduisant le rappel. Le F1-score, en tant que moyenne harmonique, ne peut être élevé que si les deux, précision et rappel, sont élevés, forçant ainsi le modèle à maintenir un équilibre entre eux."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0IICpfy412v"
      },
      "outputs": [],
      "source": [
        "def f1_score(y_true, y_pred):\n",
        "    y_pred = tf.math.round(y_pred)  # Arrondir les prédictions à 0 ou 1\n",
        "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, 'float'), axis=0)\n",
        "    tn = tf.reduce_sum(tf.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\n",
        "    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, 'float'), axis=0)\n",
        "    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + tf.keras.backend.epsilon())\n",
        "    r = tp / (tp + fn + tf.keras.backend.epsilon())\n",
        "\n",
        "    f1 = 2 * p * r / (p + r + tf.keras.backend.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return tf.reduce_mean(f1)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',  # Utiliser 'binary_crossentropy' pour la classification binaire\n",
        "              metrics=['accuracy', f1_score])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dZbzAL2412w"
      },
      "outputs": [],
      "source": [
        "# Pour la classification binaire, assurez-vous que y_train et y_test sont binaires (0 ou 1)\n",
        "# Entrainement du modèle CNN\n",
        "history = model.fit(X_train_reshaped, y_train, epochs=50, batch_size=64, validation_split=0.2, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofHLcySb412w"
      },
      "source": [
        "Le modèle a pour f1-score 87.9%. Nous allons faire des tests manuels pour optimiser la performance du modèle CNN. Voir fichier **tp2-cnn_tests.ipynb** pour plus de détails. Nous avons utilisé un différent fichier pour faire les tests sur CNN parce que ça nécessitait trop de mémoire et c'est plus optimal et intéressant de faire ces calculs sur Colab. Nous avons obtenu le meilleur résultat pour ces valeurs de paramètres : \"Filtres: 64, Taille de noyau: 2, Taux de dropout: 0.5, Optimiseur: rmsprop, LR: 0.001 \" pour un F1-Score: 0.8. Nous allons tous de même présenter ces résultats en détails dans la partie suivante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTvzzkuP412w"
      },
      "source": [
        "Le nombre de Filtres est égal à 64. Le nombre de filtres dans une couche convolutive détermine combien de caractéristiques uniques le modèle peut extraire à chaque couche. Un total de 64 filtres permet au modèle de capturer une grande variété de caractéristiques sans être trop complexe, ce qui pourrait mener à un surapprentissage, ni trop simple, ce qui limiterait sa capacité à apprendre des caractéristiques fines des données.\n",
        "\n",
        "La taille de noyau est égale à 2. La taille du noyau affecte la quantité d'information que le filtre considère à chaque étape de convolution. Une taille de noyau de 2 permet au modèle de détecter des motifs très locaux, ce qui est particulièrement utile pour capter les relations à court terme dans les données, comme les commandes qui se suivent d'un seul client ou les tendances de commandes des clients sur une période précise.\n",
        "\n",
        "Le taux de dropout est égal à 0.5. Le dropout est une technique de régularisation utilisée pour éviter le surapprentissage en \"éteignant\" aléatoirement une proportion de neurones durant l'entraînement. Un taux de 0.5 signifie que la moitié des activations sont annulées à chaque mise à jour. Cela force le réseau à apprendre des caractéristiques robustes en utilisant seulement une fraction des neurones, améliorant ainsi la généralisation.\n",
        "\n",
        "L'optimiseur optimal est RMSprop. Il ajuste le taux d'apprentissage de manière adaptative pour chaque paramètre. Il est particulièrement efficace pour naviguer dans des paysages de gradients complexes et pour des problèmes où le gradient peut changer de manière abrupte, ce qui le rend bien adapté à de nombreux types de données de CNN.\n",
        "\n",
        "LR (Learning Rate) est égal à 0.001. Le taux d'apprentissage détermine la taille des pas que l'optimiseur prend dans le paysage des gradients. Un taux de 0.001 est suffisamment petit pour permettre des ajustements précis dans les poids du modèle sans sauter par-dessus des minima locaux, mais assez grand pour assurer une convergence rapide. Ce taux est souvent choisi comme point de départ pour de nombreux types de réseaux neuronaux."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Construction du modèle CNN avec les paramètres spécifiés\n",
        "model_cnn = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model_cnn.compile(optimizer=RMSprop(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Define early stopping criteria\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Entraînement du modèle CNN avec early stopping\n",
        "history = model_cnn.fit(X_train_reshaped, y_train, epochs=100, batch_size=64, verbose=0,\n",
        "                        validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Prédiction sur l'ensemble de test\n",
        "predictions_cnn = model_cnn.predict(X_test_reshaped)\n",
        "predictions_cnn = (predictions_cnn > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "PKHA6ktb-97A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul des métriques\n",
        "accuracy = accuracy_score(y_test, predictions_cnn)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, predictions_cnn, average='macro')\n",
        "# y_test_single_label = np.argmax(y_test, axis=1)\n",
        "# predictions_cnn_single_label = np.argmax(predictions_cnn, axis=1)\n",
        "# conf_matrix = confusion_matrix(y_test_single_label, predictions_cnn_single_label)\n",
        "\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "confusion_matrix = multilabel_confusion_matrix(y_test, predictions_cnn, labels=[0, 1])\n",
        "# Affichage des métriques\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Affichage de la matrice de confusion\n",
        "print(\"Matrice de confusion :\")\n",
        "print(confusion_matrix)\n",
        "\n",
        "# Affichage des courbes d'apprentissage\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Courbe d'exactitude\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "plt.title('Courbe d\\'exactitude')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Courbe de perte\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.title('Courbe de perte')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eJVCQyFvGMdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbIcpLjy412x"
      },
      "outputs": [],
      "source": [
        "!pip install lazypredict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKmLFoNL412x"
      },
      "source": [
        "## Entraînement de modèles Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6jDOBMV412x"
      },
      "source": [
        "Résultat préliminaire avec la bibliothèque LazyPredict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4dEt5pv412y"
      },
      "outputs": [],
      "source": [
        "from lazypredict.Supervised import LazyClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initialisation de LazyClassifier pour comparer les modèles de classification\n",
        "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
        "models, predictions = clf.fit(X_train, X_test, y_train_t, y_test_t)\n",
        "\n",
        "# Affichage des scores F1 des modèles\n",
        "print(models['F1 Score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8SW1Hy3412y"
      },
      "source": [
        "Nous remarquons que les F1-scores sont assez proches. Le LGBM (Light Gradient Boosting Model) est le modèle le plus performant avec 0.79 de f1-score. Il est suivi par plusieurs modèles ayant comme f1-score 0.78. Nous allons en choisir quelques uns pour les optimiser avec GridSearchCV. Pour cela, nous allons travailler sur les modèles LGBMClassifier, AdaBoostClassifier, LinearDiscriminantAnalysis et RandomForestClassifier. Lazypredict permet en effet d'avoir des résultats préliminaires sur une multitude de modèles de Machine Learning (utilisant plusieurs techniques comme le Bagging, Boosting et autres techniques d'apprentissage).\n",
        "Nous allons tout de même commencer par optimiser le modèle SVM puisque c'est demandé dans l'énoncé."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOipZisn412y"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous implémentons la méthode de classification Machine à vecteur de support, avec un gridsearch pour la recherche des meilleurs hyper-parametres (Le noyeau, gamma et la constante C)."
      ],
      "metadata": {
        "id": "wsywiBzHE6RD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvXnqBtF412z"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category = ConvergenceWarning)\n",
        "\n",
        "#Différentes valeurs pour différents hyperparamètres du modèle SVM\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # Plus C est élevé, moins la régularisation est forte\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf', 'poly']  # différents noyaux\n",
        "}\n",
        "\n",
        "svc = SVC(max_iter=200)  # Initialiser le modèle SVM\n",
        "\n",
        "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, refit=True, verbose=2, cv=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'entrainement de la grille de recherche pour trouver le meilleur modèle avec les bons hyper-parametres."
      ],
      "metadata": {
        "id": "jvPp1fIHFGXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU8g_7KB412z"
      },
      "outputs": [],
      "source": [
        "grid_search.fit(X_train, y_train_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dL8WDsgbFYKO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLBBntpH4120"
      },
      "outputs": [],
      "source": [
        "print(f\"Meilleurs paramètres: {grid_search.best_params_}\")\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Prédiction et rapport de classification sur l'ensemble de test\n",
        "predictions = best_model.predict(X_test)\n",
        "print(classification_report(y_test_t, predictions))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}